{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming - Value Iteration and Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai 1 avec une programmation dynamique totalement observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation de l'environnement *Baba is you*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.0 (SDL 2.0.12, python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame as pg\n",
    "from stateHandler import step, printRules, simplify, isWinState, getActionList, stateToUp\n",
    "from spritesheet import Spritesheet\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Induction and Value Iteration (**Bellman**), cf TD4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the state and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = getActionList()\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_labels = {\n",
    "    0: \"Move Up\",\n",
    "    1: \"Move Right\",\n",
    "    2: \"Move Down\",\n",
    "    3: \"Move Left\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains a function that can be used to display policies with the *Baba is you* environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    actions_src = [\"{}={}\".format(action, action_labels[action].replace(\"Move \", \"\")) for action in actions]\n",
    "    title = \"Policy (\" + \", \".join(actions_src) + \")\"\n",
    "    states_display(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "\n",
    "value_function_history = []\n",
    "delta_history = []\n",
    "\n",
    "def value_iteration(gamma=0.95, epsilon=0.001, display=False):\n",
    "    v_array = np.zeros(len(states))   # Initial value function\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        #if display:\n",
    "        #    states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "        value_function_history.append(v_array.copy())\n",
    "        \n",
    "        delta = 0.\n",
    "        \n",
    "        for S in states :\n",
    "            if is_final_array[S] :\n",
    "                v_array[S] = reward_array[S]\n",
    "            else :\n",
    "                markov = [] #Markov process to compute the best action\n",
    "                for A in actions :\n",
    "                    S_prime = stateToUp(S, A)\n",
    "                    markov.append(value_function_history[-1][S_prime])\n",
    "                    \n",
    "                v_array[S] = reward_array[S] + gamma * max(markov)\n",
    "            \n",
    "            if (abs(v_array[S]-value_function_history[-1][S]) > delta) :\n",
    "                delta = abs(v_array[S]-value_function_history[-1][S])\n",
    "\n",
    "        delta_history.append(delta)\n",
    "        \n",
    "        if (delta != 0)and(delta < epsilon*(1-gamma)/gamma):\n",
    "            stop = True\n",
    "    \n",
    "    return v_array\n",
    "        \n",
    "v_array = value_iteration(display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
